## Copyright 2022 Lee Joo-Hyun
## This is offical code of the BC-Res2Net

import math

import einops
import torch
from torch import nn

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def z_norm(x, dims, eps: float = 1e-8):
    mean = x.mean(dim=dims, keepdim=True)
    var = torch.var(x, dim=dims, keepdim=True, unbiased=False)
    value = (x - mean) / (var + eps).sqrt()
    return value


class FreqInstanceNorm(nn.Module):
    def __init__(self):
        super(FreqInstanceNorm, self).__init__()

    @staticmethod
    def _freq_instance_norm(x, eps: float = 1e-8):
        return z_norm(x=x, dims=[1, -1], eps=eps)  # Channel, Temporal

    def forward(self, inputs):
        return self._freq_instance_norm(x=inputs)


class ResNorm(nn.Module):
    def __init__(self,
                 lam: float = 0.1):
        super(ResNorm, self).__init__()
        self.norm = FreqInstanceNorm()
        self.lam = lam  # lambda

    def forward(self, inputs):
        return inputs * self.lam + self.norm(inputs)


class SubSpectralNorm(nn.Module):
    def __init__(self,
                 input_dims: int,
                 num_sub_bands: int,
                 affine: bool = True,
                 eps: float = 1e-5):
        super(SubSpectralNorm, self).__init__()
        self.num_sub_bands = num_sub_bands
        self.eps = eps
        self.bn = nn.BatchNorm2d(
            num_features=input_dims * num_sub_bands,
            affine=affine,
        )

    def forward(self, inputs):
        batch_size, channels, height, width = inputs.size()
        out = inputs.view(
            batch_size,
            channels * self.num_sub_bands,
            height // self.num_sub_bands,
            width
        )
        out = self.bn(out)
        return out.view(batch_size, channels, height, width)


class Normalize(nn.Module):
    def __init__(self,
                 in_channels: int,
                 norm_type: 'str' = 'bn',
                 eps: float = 1e-6):
        super(Normalize, self).__init__()
        if norm_type == 'ln':
            self.norm = nn.GroupNorm(num_groups=1, num_channels=in_channels, eps=eps)
        elif norm_type == 'in':
            self.norm = nn.GroupNorm(num_groups=in_channels, num_channels=in_channels, eps=eps)
        else:
            self.norm = nn.BatchNorm2d(num_features=in_channels, eps=eps)

    def forward(self, inputs):
        return self.norm(inputs)


class BCResNetF2Unit(nn.Module):
    def __init__(self,
                 kernel_size,
                 num_channels: int,
                 num_sub_bands: int = 4,
                 bias: bool = False):
        super(BCResNetF2Unit, self).__init__()

        self.freq_dw_conv = nn.Conv2d(
            in_channels=num_channels,
            out_channels=num_channels,
            kernel_size=(kernel_size, 1),
            padding=(kernel_size // 2, 0),
            groups=num_channels,
            bias=bias
        )

        self.sub_spec_norm = SubSpectralNorm(
            input_dims=num_channels,
            num_sub_bands=num_sub_bands
        )

    def forward(self, inputs):
        out = self.freq_dw_conv(inputs)
        out = self.sub_spec_norm(out)
        return out


class BCResNetF1Unit(nn.Module):
    def __init__(self,
                 num_channels: int,
                 kernel_size: int,
                 norm_type: str = 'bn',
                 bias: bool = False):
        super(BCResNetF1Unit, self).__init__()

        self.temp_dw_conv = nn.Conv2d(
            in_channels=num_channels,
            out_channels=num_channels,
            kernel_size=(1, kernel_size),
            padding=(0, kernel_size // 2),
            groups=num_channels,
            bias=bias
        )
        self.norm = Normalize(in_channels=num_channels, norm_type=norm_type)
        self.swish = nn.SiLU()

    def forward(self, inputs):
        out = self.temp_dw_conv(inputs)
        out = self.norm(out)
        out = self.swish(out)
        return out


class BroadcastBlock(nn.Module):
    def __init__(self,
                 input_dims: int,
                 num_sub_bands: int = 4,
                 norm_type: str = 'bn',
                 bias: bool = False,
                 dropout: float = .1,
                 **kwargs):
        super(BroadcastBlock, self).__init__()

        self.freq_dw_conv = BCResNetF2Unit(
            kernel_size=3,
            num_channels=input_dims,
            num_sub_bands=num_sub_bands,
            bias=bias
        )

        self.temp_dw_conv = BCResNetF1Unit(
            num_channels=input_dims,
            kernel_size=3,
            norm_type=norm_type,
            bias=bias
        )

        self.point_wise = nn.Conv2d(
            in_channels=input_dims,
            out_channels=input_dims,
            kernel_size=(1, 1),
            bias=bias
        )
        self.channel_drop = nn.Dropout2d(p=dropout)
        self.relu = nn.ReLU()

    def forward(self, inputs):
        identity = inputs

        # f_2
        out = self.freq_dw_conv(inputs)

        auxilary = out

        # Frequency average pooling
        out = out.mean(dim=-2, keepdim=True)

        # f_1
        out = self.temp_dw_conv(out)
        out = self.point_wise(out)
        out = self.channel_drop(out)

        out = out.expand_as(identity) + identity + auxilary
        out = self.relu(out)

        return out


class TransitionBlock(nn.Module):

    def __init__(self,
                 input_dims: int,
                 output_dims: int,
                 num_sub_bands: int = 4,
                 dropout: float = .1,
                 norm_type: str = 'bn',
                 bias: bool = False,
                 **kwargs):
        super(TransitionBlock, self).__init__()

        self.point_wise_1 = nn.Conv2d(
            in_channels=input_dims,
            out_channels=output_dims,
            kernel_size=(1, 1),
            bias=bias
        )
        self.norm = Normalize(in_channels=output_dims, norm_type=norm_type)
        self.relu_1 = nn.ReLU()

        self.freq_dw_conv = BCResNetF2Unit(
            kernel_size=3,
            num_channels=output_dims,
            num_sub_bands=num_sub_bands,
            bias=bias
        )

        self.temp_dw_conv = BCResNetF1Unit(
            num_channels=output_dims,
            kernel_size=3,
            norm_type=norm_type,
            bias=bias
        )

        self.point_wise_2 = nn.Conv2d(
            in_channels=output_dims,
            out_channels=output_dims,
            kernel_size=(1, 1),
            bias=bias
        )
        self.channel_drop = nn.Dropout2d(p=dropout)

        self.relu_2 = nn.ReLU()

    def forward(self, inputs):
        # f_2
        out = self.point_wise_1(inputs)
        out = self.norm(out)
        out = self.relu_1(out)
        out = self.freq_dw_conv(out)
        auxilary = out

        out = out.mean(dim=-2, keepdim=True)  # frequency average pooling

        # f_1
        out = self.temp_dw_conv(out)
        out = self.point_wise_2(out)
        out = self.channel_drop(out)

        out = out + auxilary

        out = self.relu_2(out)

        return out


class ECABlock(nn.Module):
    """
    Constructs a ECA module.
        Args: channel: Number of channels of the input feature map
        k_size: Adaptive selection of kernel size
    """

    def __init__(self, in_channels: int, gamma: int = 2, b: int = 1, bias: bool = False):
        super(ECABlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        t = int(abs((math.log(in_channels, 2) + b) / gamma))
        k = t if t % 2 else t + 1

        self.eca_conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=k, padding=k // 2, bias=bias)
        self.sigmoid = nn.Sigmoid()

    def forward(self, inputs):
        # feature descriptor on the global spatial information
        y = self.avg_pool(inputs)
        y = einops.rearrange(
            tensor=y,
            pattern='batch_size channels width length -> '
                    'batch_size (width length) channels',
        )

        # Two different branches of ECA module
        y = self.eca_conv(y)

        # Multi-scale information fusion
        y = self.sigmoid(y)

        y = einops.rearrange(
            tensor=y,
            pattern='batch_size (width length) channels  -> '
                    'batch_size channels width length',
            length=1,
            width=1,
        )

        return inputs * y.expand_as(inputs)


class SEBlock(nn.Module):
    def __init__(self, in_channels, sq_ratio: float = .5, bias=False):
        super(SEBlock, self).__init__()
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, int(in_channels * sq_ratio), kernel_size=1, bias=bias),
            nn.ReLU(),
            nn.Conv2d(int(in_channels * sq_ratio), in_channels, kernel_size=1, bias=bias),
            nn.Sigmoid(),
        )

    def forward(self, inputs):
        x = self.se(inputs)
        return inputs * x


class EfficientFreqChannelAttention(nn.Module):
    def __init__(self,
                 in_channels: int,
                 gamma: int = 2,
                 b: int = 1,
                 bias: bool = False):
        super(EfficientFreqChannelAttention, self).__init__()
        t = int(abs((math.log(in_channels, 2) + b) / gamma))
        k = t if t % 2 else t + 1
        self.pooling = nn.AdaptiveAvgPool2d((None, 1))
        self.eca_conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=k, padding=k // 2, bias=bias)
        self.sigmoid = nn.Sigmoid()

    def forward(self, inputs):
        x = self.pooling(inputs)

        # B C F 1 -> B 1 F C
        x = einops.rearrange(
            tensor=x,
            pattern='batch_size channels width length -> batch_size length width channels',
        )

        x = self.eca_conv(x)

        # B 1 F C -> B C F 1
        x = einops.rearrange(
            tensor=x,
            pattern='batch_size length width channels -> batch_size channels width length',
        )

        x = self.sigmoid(x)
        return inputs * x.expand_as(inputs)


class FreqChannelAttention(nn.Module):
    def __init__(self, in_channels: int, sq_ratio: float = .5, bias=False):
        super(FreqChannelAttention, self).__init__()
        squeeze_channels = int(in_channels * sq_ratio)
        squeeze_channels = squeeze_channels + 1 if squeeze_channels % 2 else squeeze_channels
        self.fca = nn.Sequential(
            nn.AdaptiveAvgPool2d((None, 1)),
            nn.Conv2d(in_channels, squeeze_channels, kernel_size=1, bias=bias),
            nn.ReLU(),
            nn.Conv2d(squeeze_channels, in_channels, kernel_size=1, bias=bias),
            nn.Sigmoid(),
        )

    def forward(self, inputs):
        x = self.fca(inputs)
        return inputs * x.expand_as(inputs)


class Res2NetUnit(nn.Module):
    def __init__(self,
                 num_channels: int,
                 padding,
                 kernel_size: int = 3,
                 scales: int = 4,
                 bias: bool = False,
                 norm_type: str = 'bn',
                 dropout: float = .1,
                 **kwargs):
        super(Res2NetUnit, self).__init__()
        self.scales = scales
        self.conv_list = nn.ModuleList([
            nn.Conv2d(
                in_channels=num_channels // scales,
                out_channels=num_channels // scales,
                kernel_size=kernel_size,
                padding=padding,
                groups=num_channels // scales,
                bias=bias
            ) for _ in range(scales - 1)
        ])
        self.norm_list = nn.ModuleList([
            Normalize(
                in_channels=num_channels // scales,
                norm_type=norm_type,
            ) for _ in range(scales - 1)
        ])
        self.common_activation = nn.ReLU()
        self.se = SEBlock(in_channels=num_channels)

        self.dropout = None
        if dropout > 0.:
            self.dropout = nn.Dropout2d(p=dropout)

    def forward(self, inputs):
        chunked = torch.chunk(inputs, self.scales, dim=1)
        out_list = []
        for idx, chunk in enumerate(chunked):
            if idx == 0:
                out = chunk
            else:
                if idx == 1:
                    out = self.conv_list[idx - 1](chunk)
                else:
                    out = self.conv_list[idx - 1](chunk + out_list[-1])
                out = self.norm_list[idx - 1](out)
                out = self.common_activation(out)
            out_list.append(out)
        out = torch.cat(out_list, dim=1)
        out = self.se(out)

        if self.dropout is not None:
            out = self.dropout(out)

        return out


class MFARes2NetBlock(nn.Module):
    def __init__(self,
                 in_channels: int,
                 kernel_size: int = 3,
                 padding: int = 3 // 2,
                 res2net_scales: int = 4,
                 bias: bool = False,
                 norm_type: str = 'bn',
                 dropout: float = .1,
                 **kwargs):
        super(MFARes2NetBlock, self).__init__()
        self.bottleneck = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=in_channels * res2net_scales,
                kernel_size=(1, 1),
                bias=bias,
            ),
            Normalize(
                in_channels=in_channels * res2net_scales,
                norm_type=norm_type
            ),
            nn.ReLU(),
        )

        self.res2net = FreqAttnRes2NetUnit(
            num_channels=in_channels * res2net_scales,
            padding=padding,
            kernel_size=kernel_size,
            scales=res2net_scales,
            bias=bias,
            norm_type=norm_type,
        )

        self.inv_bottleneck = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels * res2net_scales,
                out_channels=in_channels,
                kernel_size=(1, 1),
                bias=bias,
            ),
            Normalize(
                in_channels=in_channels,
                norm_type=norm_type
            ),
        )

        self.se = SEBlock(in_channels=in_channels, bias=bias)

        self.dropout = None
        if dropout > 0.:
            self.dropout = nn.Dropout2d(p=dropout)

        self.relu = nn.ReLU()

    def forward(self, inputs):
        residual = inputs

        out = self.bottleneck(inputs)
        out = self.res2net(out)
        out = self.inv_bottleneck(out)
        out = self.se(out)

        if self.dropout is not None:
            out = self.dropout(out)

        out = out + residual
        out = self.relu(out)

        return out


class EfficientMFARes2NetBlock(nn.Module):
    def __init__(self,
                 in_channels: int,
                 kernel_size: int = 3,
                 padding: int = 3 // 2,
                 res2net_scales: int = 4,
                 bias: bool = False,
                 norm_type: str = 'bn',
                 dropout: float = .1,
                 **kwargs):
        super(EfficientMFARes2NetBlock, self).__init__()
        self.bottleneck = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=in_channels * res2net_scales,
                kernel_size=(1, 1),
                bias=bias,
            ),
            Normalize(
                in_channels=in_channels * res2net_scales,
                norm_type=norm_type
            ),
            nn.ReLU(),
        )

        self.res2net = EfficientFreqAttnRes2NetUnit(
            num_channels=in_channels * res2net_scales,
            padding=padding,
            kernel_size=kernel_size,
            scales=res2net_scales,
            bias=bias,
            norm_type=norm_type,
        )

        self.inv_bottleneck = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels * res2net_scales,
                out_channels=in_channels,
                kernel_size=(1, 1),
                bias=bias,
            ),
            Normalize(
                in_channels=in_channels,
                norm_type=norm_type
            ),
        )

        self.se = ECABlock(in_channels=in_channels)

        self.dropout = None
        if dropout > 0.:
            self.dropout = nn.Dropout2d(p=dropout)

        self.relu = nn.ReLU()

    def forward(self, inputs):
        residual = inputs

        out = self.bottleneck(inputs)
        out = self.res2net(out)
        out = self.inv_bottleneck(out)
        out = self.se(out)

        if self.dropout is not None:
            out = self.dropout(out)

        out = out + residual
        out = self.relu(out)

        return out


class FreqAttnRes2NetUnit(nn.Module):
    def __init__(self,
                 num_channels: int,
                 padding,
                 kernel_size: int = 3,
                 scales: int = 4,
                 bias: bool = False,
                 norm_type: str = 'bn',
                 **kwargs):
        super(FreqAttnRes2NetUnit, self).__init__()
        self.scales = scales
        self.conv_list = nn.ModuleList([
            nn.Conv2d(
                in_channels=num_channels // scales,
                out_channels=num_channels // scales,
                kernel_size=kernel_size,
                padding=padding,
                groups=num_channels // scales,
                bias=bias
            ) for _ in range(scales - 1)
        ])

        self.norm_list = nn.ModuleList([
            Normalize(
                in_channels=num_channels // scales,
                norm_type=norm_type,
            ) for _ in range(scales - 1)
        ])

        self.freq_attn_list = nn.ModuleList([
            FreqChannelAttention(
                in_channels=num_channels // scales,
                bias=bias,
            ) for _ in range(scales)
        ])

        self.common_activation = nn.ReLU()

    def forward(self, inputs):
        chunked = torch.chunk(inputs, self.scales, dim=1)
        out_list = []
        for idx, chunk in enumerate(chunked):
            if idx == 0:
                out = chunk
            else:
                if idx == 1:
                    out = self.conv_list[idx - 1](chunk)
                else:
                    out = self.conv_list[idx - 1](chunk + out_list[-1])
                out = self.norm_list[idx - 1](out)
                out = self.common_activation(out)
            out = self.freq_attn_list[idx](out)
            out_list.append(out)
        out = torch.cat(out_list, dim=1)
        return out


class EfficientFreqAttnRes2NetUnit(nn.Module):
    def __init__(self,
                 num_channels: int,
                 padding,
                 kernel_size: int = 3,
                 scales: int = 4,
                 bias: bool = False,
                 norm_type: str = 'bn',
                 **kwargs):
        super(EfficientFreqAttnRes2NetUnit, self).__init__()
        self.scales = scales
        self.conv_list = nn.ModuleList([
            nn.Conv2d(
                in_channels=num_channels // scales,
                out_channels=num_channels // scales,
                kernel_size=kernel_size,
                padding=padding,
                groups=num_channels // scales,
                bias=bias
            ) for _ in range(scales - 1)
        ])

        self.norm_list = nn.ModuleList([
            Normalize(
                in_channels=num_channels // scales,
                norm_type=norm_type,
            ) for _ in range(scales - 1)
        ])

        self.freq_attn_list = nn.ModuleList([
            EfficientFreqChannelAttention(
                in_channels=num_channels // scales,
                bias=bias,
            ) for _ in range(scales)
        ])

        self.common_activation = nn.ReLU()

    def forward(self, inputs):
        chunked = torch.chunk(inputs, self.scales, dim=1)
        out_list = []
        for idx, chunk in enumerate(chunked):
            if idx == 0:
                out = chunk
            else:
                if idx == 1:
                    out = self.conv_list[idx - 1](chunk)
                else:
                    out = self.conv_list[idx - 1](chunk + out_list[-1])
                out = self.norm_list[idx - 1](out)
                out = self.common_activation(out)
            out = self.freq_attn_list[idx](out)
            out_list.append(out)
        out = torch.cat(out_list, dim=1)
        return out


class BCRes2NetF2Unit(nn.Module):
    def __init__(self,
                 kernel_size,
                 padding,
                 num_channels: int,
                 num_sub_bands: int = 4,
                 scales: int = 4,
                 bias: bool = False):
        super(BCRes2NetF2Unit, self).__init__()
        self.scales = scales

        self.conv_list = nn.ModuleList([
            nn.Conv2d(
                in_channels=num_channels // scales,
                out_channels=num_channels // scales,
                kernel_size=kernel_size,
                padding=padding,
                groups=num_channels // scales,
                bias=bias
            ) for _ in range(scales - 1)
        ])

        self.sub_spec_norm_list = nn.ModuleList([
            SubSpectralNorm(
                input_dims=num_channels // scales,
                num_sub_bands=num_sub_bands
            ) for _ in range(scales - 1)
        ])

    def forward(self, inputs):
        chunked = torch.chunk(inputs, self.scales, dim=1)
        out_list = []
        for idx, chunk in enumerate(chunked):
            if idx == 0:
                out = chunk
            else:
                if idx == 1:
                    out = self.conv_list[idx - 1](chunk)
                else:
                    out = self.conv_list[idx - 1](chunk + out_list[-1])
                out = self.sub_spec_norm_list[idx - 1](out)
            out_list.append(out)
        out = torch.cat(out_list, dim=1)
        return out


class BCRes2NetF1Unit(nn.Module):
    def __init__(self,
                 kernel_size,
                 padding,
                 num_channels: int,
                 scales: int = 4,
                 norm_type: str = 'bn',
                 bias: bool = False):
        super(BCRes2NetF1Unit, self).__init__()
        self.scales = scales

        self.conv_list = nn.ModuleList([
            nn.Conv2d(
                in_channels=num_channels // scales,
                out_channels=num_channels // scales,
                kernel_size=kernel_size,
                padding=padding,
                groups=num_channels // scales,
                bias=bias
            ) for _ in range(scales - 1)
        ])

        self.norm_list = nn.ModuleList([
            Normalize(
                in_channels=num_channels // scales,
                norm_type=norm_type,
            ) for _ in range(scales - 1)
        ])

        self.common_act = nn.SiLU()

    def forward(self, inputs):
        chunked = torch.chunk(inputs, self.scales, dim=1)
        out_list = []
        for idx, chunk in enumerate(chunked):
            if idx == 0:
                out = chunk
            else:
                if idx == 1:
                    out = self.conv_list[idx - 1](chunk)
                else:
                    out = self.conv_list[idx - 1](chunk + out_list[-1])
                out = self.norm_list[idx - 1](out)
                out = self.common_act(out)
            out_list.append(out)
        out = torch.cat(out_list, dim=1)
        return out


class Broadcast2Block(nn.Module):
    def __init__(self,
                 input_dims: int,
                 num_sub_bands: int = 4,
                 bias: bool = False,
                 res2net_scales: int = 4,
                 dropout: float = .1,
                 norm_type: str = 'bn',
                 **kwargs):
        super(Broadcast2Block, self).__init__()
        self.f2 = BCRes2NetF2Unit(
            kernel_size=(3, 1),
            padding=(1, 0),
            num_channels=input_dims,
            num_sub_bands=num_sub_bands,
            scales=res2net_scales,
            bias=bias
        )

        self.f1 = BCRes2NetF1Unit(
            kernel_size=(1, 3),
            padding=(0, 1),
            num_channels=input_dims,
            norm_type=norm_type,
            scales=res2net_scales,
            bias=bias,
        )

        self.point_wise = nn.Conv2d(
            in_channels=input_dims,
            out_channels=input_dims,
            kernel_size=(1, 1),
            bias=bias
        )
        self.channel_drop = nn.Dropout2d(p=dropout)

        self.relu = nn.ReLU()

    def forward(self, inputs):
        identity = inputs

        out = self.f2(inputs)
        auxiliary = out

        # Frequency average pooling
        out = out.mean(dim=-2, keepdim=True)

        # f_1
        out = self.f1(out)

        out = self.point_wise(out)
        out = self.channel_drop(out)

        out = out.expand_as(identity) + identity + auxiliary

        out = self.relu(out)
        return out


class Transition2Block(nn.Module):

    def __init__(self,
                 input_dims: int,
                 output_dims: int,
                 num_sub_bands: int = 4,
                 res2net_scales: int = 4,
                 dropout: float = .1,
                 norm_type: str = 'bn',
                 bias: bool = False,
                 **kwargs):
        super(Transition2Block, self).__init__()

        self.point_wise_1 = nn.Conv2d(
            in_channels=input_dims,
            out_channels=output_dims,
            kernel_size=(1, 1),
            stride=(1, 1),
            bias=bias
        )
        self.norm = Normalize(in_channels=output_dims, norm_type=norm_type)
        self.relu_1 = nn.ReLU()

        self.f2 = BCRes2NetF2Unit(
            kernel_size=(3, 1),
            padding=(1, 0),
            num_channels=output_dims,
            num_sub_bands=num_sub_bands,
            scales=res2net_scales,
            bias=bias
        )

        self.f1 = BCRes2NetF1Unit(
            kernel_size=(1, 3),
            padding=(0, 1),
            num_channels=output_dims,
            scales=res2net_scales,
            norm_type=norm_type,
            bias=bias,
        )

        self.point_wise_2 = nn.Conv2d(
            in_channels=output_dims,
            out_channels=output_dims,
            kernel_size=(1, 1),
            bias=bias
        )
        self.channel_drop = nn.Dropout2d(p=dropout)

        self.relu_2 = nn.ReLU()

    def forward(self, inputs):
        out = self.point_wise_1(inputs)
        out = self.norm(out)
        out = self.relu_1(out)

        # f_2
        out = self.f2(out)
        auxiliary = out

        out = out.mean(dim=-2, keepdim=True)  # frequency average pooling

        # f_1
        out = self.f1(out)
        out = self.point_wise_2(out)
        out = self.channel_drop(out)

        out = out + auxiliary

        out = self.relu_2(out)

        return out


class BCRes2NetF1ModUnit(nn.Module):
    def __init__(self,
                 kernel_size,
                 padding,
                 num_channels: int,
                 scales: int = 4,
                 bias: bool = False):
        super(BCRes2NetF1ModUnit, self).__init__()

        self.scales = scales
        self.conv_list = nn.ModuleList([
            nn.Conv2d(
                in_channels=num_channels // scales,
                out_channels=num_channels // scales,
                kernel_size=kernel_size,
                padding=padding,
                groups=num_channels // scales,
                bias=bias
            ) for _ in range(scales - 1)
        ])
        self.common_act = nn.ReLU()

    def forward(self, inputs):
        chunked = torch.chunk(inputs, self.scales, dim=1)
        out_list = []
        for idx, chunk in enumerate(chunked):
            if idx == 0:
                out = chunk
            else:
                if idx == 1:
                    out = self.conv_list[idx - 1](chunk)
                else:
                    out = self.conv_list[idx - 1](chunk + out_list[-1])
                out = self.common_act(out)
            out_list.append(out)
        out = torch.cat(out_list, dim=1)
        return out


class Broadcast2BlockMod(nn.Module):
    def __init__(self,
                 input_dims: int,
                 num_sub_bands: int = 4,
                 bias: bool = False,
                 res2net_scales: int = 4,
                 dropout: float = .1,
                 norm_type: str = 'bn',
                 **kwargs):
        super(Broadcast2BlockMod, self).__init__()
        self.f2 = BCRes2NetF2Unit(
            kernel_size=(3, 1),
            padding=(1, 0),
            num_channels=input_dims,
            num_sub_bands=num_sub_bands,
            scales=res2net_scales,
            bias=bias
        )

        self.f1 = BCRes2NetF1ModUnit(
            kernel_size=(1, 3),
            padding=(0, 1),
            num_channels=input_dims,
            scales=res2net_scales,
            bias=bias,
        )

        self.point_wise = nn.Conv2d(
            in_channels=input_dims,
            out_channels=input_dims,
            kernel_size=(1, 1),
            bias=bias
        )

        self.channel_drop = nn.Dropout2d(p=dropout)

        self.norm = Normalize(in_channels=input_dims, norm_type=norm_type)
        self.relu = nn.ReLU()

    def forward(self, inputs):
        identity = inputs

        out = self.f2(inputs)
        auxiliary = out

        # Frequency average pooling
        out = out.mean(dim=-2, keepdim=True)

        # f_1
        out = self.f1(out)
        out = self.point_wise(out)
        out = self.channel_drop(out)

        out = out.expand_as(identity) + identity + auxiliary

        out = self.norm(out)
        out = self.relu(out)
        return out


class Transition2BlockMod(nn.Module):
    def __init__(self,
                 input_dims: int,
                 output_dims: int,
                 num_sub_bands: int = 4,
                 res2net_scales: int = 4,
                 dropout: float = .1,
                 norm_type: str = 'bn',
                 bias: bool = False,
                 **kwargs):
        super(Transition2BlockMod, self).__init__()
        self.point_wise_1 = nn.Conv2d(
            in_channels=input_dims,
            out_channels=output_dims,
            kernel_size=(1, 1),
            stride=(1, 1),
            bias=bias
        )
        self.norm_1 = Normalize(in_channels=output_dims, norm_type=norm_type)
        self.relu_1 = nn.ReLU()

        self.f2 = BCRes2NetF2Unit(
            kernel_size=(3, 1),
            padding=(1, 0),
            num_channels=output_dims,
            num_sub_bands=num_sub_bands,
            scales=res2net_scales,
            bias=bias
        )

        self.f1 = BCRes2NetF1ModUnit(
            kernel_size=(1, 3),
            padding=(0, 1),
            num_channels=output_dims,
            scales=res2net_scales,
            bias=bias,
        )

        self.point_wise_2 = nn.Conv2d(
            in_channels=output_dims,
            out_channels=output_dims,
            kernel_size=(1, 1),
            bias=bias
        )
        self.channel_drop = nn.Dropout2d(p=dropout)

        self.norm_2 = Normalize(in_channels=output_dims, norm_type=norm_type)
        self.relu_2 = nn.ReLU()

    def forward(self, inputs):
        out = self.point_wise_1(inputs)
        out = self.norm_1(out)
        out = self.relu_1(out)

        # f_2
        out = self.f2(out)
        auxiliary = out
        out = out.mean(dim=-2, keepdim=True)  # frequency average pooling

        # f_1
        out = self.f1(out)
        out = self.point_wise_2(out)
        out = self.channel_drop(out)

        out = out + auxiliary
        out = self.norm_2(out)
        out = self.relu_2(out)

        return out
